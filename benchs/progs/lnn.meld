/* Training of a three-layer neural network. */
include #data/nn-data.meld
   
type route link(node, node).
type route back-link(node, node).
type linear weight(node, int, node, float).
type input(node).
type hidden(node).
type output(node).
type totalinput(node, int).
type totaloutput(node, int).
type bias(node).
type linear activated(node, int, float).
type linear oldactivated(node, int, float).
type linear expected(node, int, float).
type linear receive(node, int, float).
type linear toreceive(node, int, int, float).
type linear start(node, int).
type linear outgrad(node, node, int, float).
type linear oldweight(node, int, node, float).
type linear hiddengrad(node, node, float).
type linear missing-gradients(node, int).

//priority @cluster @random.

const lrate = 0.01.

//set-cpu(@0, 0 % @cpus).  set-cpu(@1, 1 % @cpus).  set-cpu(@2, 2 % @cpus).  set-cpu(@3, 3 % @cpus).  set-cpu(@4, 4 % @cpus).  set-cpu(@5, 5 % @cpus).  set-cpu(@6, 6 % @cpus).  set-cpu(@7, 7 % @cpus).  set-cpu(@8, 8 % @cpus).  set-cpu(@9, 9 % @cpus).  set-cpu(@10, 10 % @cpus).  set-cpu(@11, 11 % @cpus).  set-cpu(@12, 12 % @cpus).  set-cpu(@13, 13 % @cpus).  set-cpu(@14, 14 % @cpus).  set-cpu(@15, 15 % @cpus).  set-cpu(@16, 16 % @cpus).  set-cpu(@17, 17 % @cpus).  set-cpu(@18, 18 % @cpus).  set-cpu(@19, 19 % @cpus).  set-cpu(@20, 20 % @cpus).  set-cpu(@21, 21 % @cpus).  set-cpu(@22, 22 % @cpus).  set-cpu(@23, 23 % @cpus).  set-cpu(@24, 24 % @cpus).  set-cpu(@25, 25 % @cpus).  set-cpu(@26, 26 % @cpus).
set-default-priority(@3, 5.0).

!input(A),
activated(A, Id, V),
start(A, Id)
	-o {B, W | !link(A, B), weight(A, Id, B, W) | receive(B, Id, V * W), weight(A, Id, B, W)}, oldactivated(A, Id, V).

// hidden bias has no activated axiom
!hidden(A),
!bias(A),
start(A, Id)
	-o {B, W | !link(A, B), weight(A, Id, B, W) | receive(B, Id, (0.0 - 1.0) * W), oldweight(A, Id, B, W)}.

activated(A, Id, V),
!hidden(A)
	-o {B, W | !link(A, B), weight(A, Id, B, W) | receive(B, Id, V * W), oldweight(A, Id, B, W)}, oldactivated(A, Id, V).

// process signal at hidden or output layer from inputs
!hidden(A),
!bias(A),
toreceive(A, 0, Id, Acc),
!totalinput(A, T) -o
	activated(A, Id, sigmoid(Acc - 1.0)),
	toreceive(A, T, Id + 1, 0.0).

toreceive(A, 0, Id, Acc),
!totalinput(A, T)	-o 
	activated(A, Id, sigmoid(Acc)),
	toreceive(A, T, Id + 1, 0.0).

receive(A, Id, V),
toreceive(A, T, Id, Acc),
T > 0
	-o [sum => S, count => C | receive(A, Id, S) | toreceive(A, T - (C + 1), Id, Acc + V + S)].

// compare output
!output(A),
expected(A, Id, E),
activated(A, Id, G)
	-o {B | !back-link(A, B) | outgrad(B, A, Id, G * (1.0 - G) * (E - G))}.

!hidden(A),
!bias(A),
!link(A, B),
outgrad(A, B, Id, G),
oldweight(A, Id, B, W)
   -o weight(A, Id + 1, B, W + lrate * (0.0 - 1.0) * G),
      start(A, Id + 1).

!hidden(A),
!link(A, B),
outgrad(A, B, Id, G),
oldactivated(A, Id, V),
oldweight(A, Id, B, W)
	-o {C | !back-link(A, C) | hiddengrad(C, A, V * (1.0 - V) * (V * W))},
		weight(A, Id + 1, B, W + lrate * V * G).

!input(A),
missing-gradients(A, 0),
!totaloutput(A, T),
oldactivated(A, Id, _)
	-o start(A, Id + 1),
      missing-gradients(A, T).

!input(A),
!link(A, B),
missing-gradients(A, T),
oldactivated(A, Id, V),
hiddengrad(A, B, G),
weight(A, Id, B, W)
	-o oldactivated(A, Id, V),
      weight(A, Id + 1, B, W + lrate * V * G),
		missing-gradients(A, T - 1).

